# evaluate the base gpt2
# n_layer=12, n_head=12, n_embd=768
# 124M parameters
init_from = 'gpt2'

[training]
eval_iters = 500 # use more iterations to get good estimate

[data]
batch_size = 8


[logging]
wandb_log = false
