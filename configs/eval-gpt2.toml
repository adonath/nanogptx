# evaluate the base gpt2
# n_layer=12, n_head=12, n_embd=768
# 124M parameters
[training]
batch_size = 8
eval_iters = 500 # use more iterations to get good estimate
init_from = 'gpt2'

[logging]
wandb_log = false
