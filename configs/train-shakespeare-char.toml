init_from = "scratch"
seed = 9283
device = "TFRT_CPU_0"
dtype = "float32"


[training]
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 20
always_save_checkpoint = false


[training.optimizer]
gradient_accumulation_steps = 1
learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 2000
lr_decay_iters = 2000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta1 = 0.9
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small
warmup_iters = 100 # not super necessary potentially

[data]
batch_size = 12
dataset = "shakespeare"
encoding = "char"

[model]
block_size = 64 # context of up to 256 previous characters
n_layer = 4
n_head = 4
n_embd = 128
dropout_rate = 0.0
use_bias = false

[logging]
wandb_log = false # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'
