init_from = "scratch"
seed = 9283
dtype = "float32"

[sharding]
axis_names = []
axis_shapes = [1,]

[training]
log_interval = 1
eval_interval = 2000
eval_iters = 3
checkpoint_interval = 5000
show_progress = true
wandb_log = false
total_batch_size = 256

[training.optimizer]
learning_rate = 0.0006
max_iters = 600000
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0
decay_lr = true
warmup_iters = 2000
lr_decay_iters = 600000
gradient_accumulation_steps = 0
min_lr = 6e-05

[loading]
batch_size = 16
block_size = 1024
verify = true
seed = 8273
dtype = "int32"
randomize_shards = true

[loading.index]
dataset = "openwebtext"
encoding = "gpt2"
suffix = "train"

[loading.sharding]
axis_names = ["batch",]
axis_shapes = [1,]

[model]
block_size = 1024
vocab_size = 50304
n_layer = 12
n_head = 12
n_embd = 768
dropout_rate = 0.1
use_bias = true
init_std = 0.02

[logging]
wandb_log = false
wandb_project = "nanogptx"
wandb_run_name = "rebel-ratel"
wandb_tags = []
