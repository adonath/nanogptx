init_from = "scratch"
seed = 9283
device = "TFRT_CPU_0"
dtype = "float32"

[training]
log_interval = 1
eval_interval = 2000
eval_iters = 3
always_save_checkpoint = true
show_progress = true
wandb_log = false

[training.optimizer]
learning_rate = 0.0006
max_iters = 600000
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0
decay_lr = true
warmup_iters = 2000
lr_decay_iters = 600000
gradient_accumulation_steps = 40
min_lr = 6e-05

[data]
batch_size = 12
block_size = 1024
verify = true
dataset = "openwebtext"
encoding = "gpt2"
seed = 8273
dtype = "int32"
device = "TFRT_CPU_0"
suffix = "train"

[model]
block_size = 1024
vocab_size = 50304
n_layer = 12
n_head = 12
n_embd = 768
dropout_rate = 0.0
use_bias = true
init_std = 0.02

[logging]
wandb_log = false
wandb_project = "nanogptx"
wandb_run_name = "nocturnal-naiad"
