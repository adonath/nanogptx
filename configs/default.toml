#[training]
log_interval = 1
eval_interval = 2000
eval_iters = 3
always_save_checkpoint = true
init_from = "scratch"
dataset = "openwebtext"
batch_size = 12
show_progress = true

#[optimizer]
learning_rate = 0.0006
max_iters = 600000
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0
decay_lr = true
warmup_iters = 2000
lr_decay_iters = 600000
gradient_accumulation_steps = 40
min_lr = 6e-05

#[model]
block_size = 1024
vocab_size = 50304
n_layer = 12
n_head = 12
n_embd = 768
dropout_rate = 0.0
use_bias = true
init_std = 0.02

#[logging]
wandb_log = false
wandb_project = "owt"
